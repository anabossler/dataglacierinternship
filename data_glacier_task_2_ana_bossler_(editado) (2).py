# -*- coding: utf-8 -*-
"""Data Glacier Task 2 Ana Bossler (editado).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H5JsXlW-Rhyu-rsUKclEkc93IfLFpVtl
"""

import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import sklearn
# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
# %matplotlib inline

print("Versión de librería sklearn: {}".format(sklearn.__version__))

datacab = pd.read_csv('Cab_Data.csv', header=0)
datacity = pd.read_csv('City.csv', header=0)
datacustomer = pd.read_csv('Customer_ID.csv', header=0)
datatransaction = pd.read_csv('Transaction_ID.csv', header=0)

datacab.head()

#costo medio de viaje
#costo medio de viaje por km
#costo medio de viaje en función de la ciudad
#número de viajes

datacab_pink = datacab[datacab['Company'] == 'Pink Cab']
datacab_pink.mean()

datacab_yellow = datacab[datacab['Company'] == 'Yellow Cab']
datacab_yellow.mean()

datacab_pink.hist()

datacab_yellow.hist()

print(datacab['Cost of Trip'].mean())
print(datacab['KM Travelled'].mean())

counts = datacab['Company'].value_counts()
counts

datacity.head()

datacustomer.head()

#edad media
#income media
datacustomer.hist()

datatransaction.head()

#relacionar transaction ID con customer ID y compañía, para conocer por separado los valores para las dos compañías

print(f'The number of rows are {datacab.shape[0]} \nand the number of columns are {datacab.shape[1]}')

print(f'The number of rows are {datacity.shape[0]} \nand the number of columns are {datacity.shape[1]}')

print(f'The number of rows are {datacustomer.shape[0]} \nand the number of columns are {datacustomer.shape[1]}')

print(f'The number of rows are {datatransaction.shape[0]} \nand the number of columns are {datatransaction.shape[1]}')

datacab.describe()

datacity.describe()

datacustomer.describe()

datatransaction.describe()

datacab.corr()

datacustomer.corr()

datatransaction.corr()

sns.pairplot(datacab,diag_kind='kde')

sns.pairplot(datacustomer,diag_kind='kde')

sns.pairplot(datatransaction,diag_kind='kde')

def missing(df):
    total=df.isnull().sum().sort_values(ascending=False)
    percent=(df.isnull().sum()*100/df.isnull().count()).sort_values(ascending=False)

missing(datacab)

missing(datacity)

missing(datacustomer)

missing(datatransaction)

datacab.isnull().sum()

datacity.isnull().sum()

datacustomer.isnull().sum()

datatransaction.isnull().sum()

features=list(datacab.select_dtypes(exclude=['object']))
fig=plt.subplots(figsize=(15,30))
for i, j in enumerate(features):
    plt.subplot(6, 2, i+1),
    plt.subplots_adjust(hspace = 1.0)
    sns.boxplot(datacab[j])
    plt.title(j)

features=list(datacity.select_dtypes(exclude=['object']))
fig=plt.subplots(figsize=(15,30))
for i, j in enumerate(features):
    plt.subplot(6, 2, i+1),
    plt.subplots_adjust(hspace = 1.0)
    sns.boxplot(datacity[j])
    plt.title(j)

features=list(datacustomer.select_dtypes(exclude=['object']))
fig=plt.subplots(figsize=(15,30))
for i, j in enumerate(features):
    plt.subplot(6, 2, i+1),
    plt.subplots_adjust(hspace = 1.0)
    sns.boxplot(datacustomer[j])
    plt.title(j)

features=list(datatransaction.select_dtypes(exclude=['object']))
fig=plt.subplots(figsize=(15,30))
for i, j in enumerate(features):
    plt.subplot(6, 2, i+1),
    plt.subplots_adjust(hspace = 1.0)
    sns.boxplot(datatransaction[j])
    plt.title(j)

transactions = pd.merge(datacab, datatransaction, on='Transaction ID')
transactions

todo = pd.merge(transactions, datacustomer, on='Customer ID')
todo

plt.scatter(todo['Cost of Trip'], todo['Income (USD/Month)'],alpha=0.05)

plt.scatter(todo['Date of Travel'], todo['Age'],alpha=0.002)

plt.scatter(todo['Cost of Trip'], todo['KM Travelled'],alpha=0.005)

plt.hist(todo['Cost of Trip'])

todo

todo['Company'] = todo['Company'].map({'Pink Cab': 0, 'Yellow Cab': 1})
todo

todo['City'].value_counts()

todo['City']= todo['City'].map( {'NEW YORK NY': 0, 'CHICAGO IL': 1, 'LOS ANGELES CA': 2, 'WASHINGTON DC': 3,'BOSTON MA': 4, 'SAN DIEGO CA': 5, 'SILICON VALLEY': 6, 'SEATTLE WA': 7, 'ATLANTA GA': 8, 'DALLAS TX': 9, 'MIAMI FL': 10, 'AUSTIN TX': 11, 'ORANGE COUNTY': 12, 'DENVER CO': 13, 'NASHVILLE TN': 14, 'SACRAMENTO CA': 15, 'PHOENIX AZ': 16, 'TUCSON AZ': 17, 'PITTSBURGH PA': 18})#.astype(int)

todo

todo['City'].value_counts()

todo['Gender'] = todo['Gender'].map({'Female': 0, 'Male': 1})

todo

todo = todo.drop(['Payment_Mode'],axis=1)
todo

todo = todo.drop(['Transaction ID', 'Customer ID'],axis=1)
todo

target = todo.pop('Company')

import tensorflow as tf
dataset = tf.data.Dataset.from_tensor_slices((todo.values, target.values))

for feat, targ in dataset.take(5):
  print ('Features: {}, Target: {}'.format(feat, targ))

train_dataset = dataset.shuffle(len(todo)).batch(64)

def get_compiled_model():
    model = tf.keras.Sequential([
    tf.keras.layers.Dense(8, activation='relu'),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(1)
  ])

    model.compile(optimizer='adam',
                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                metrics=['accuracy'])
    return model

model = get_compiled_model()
model.fit(train_dataset, epochs=15)

predictions = model.predict('Company')
print(predictions)

datacab = pd.read_csv("/content/Cab_Data.csv", header=0, delimiter=',')

datacity = pd.read_csv("/content/City.csv", header=0, delimiter=',')

datacustomer = pd.read_csv("/content/Customer_ID.csv", header=0, delimiter=',')

datatransaction = pd.read_csv("/content/Transaction_ID.csv", header=0, delimiter=',')

transactions = pd.merge(datacab, datatransaction, on='Transaction ID')
transactions

todo = pd.merge(transactions, datacustomer, on='Customer ID')
todo

todo = pd.merge(todo, datacity, on='City')
todo

todo = todo.drop(['Payment_Mode'],axis=1)
todo

todo = todo.drop(['Transaction ID'],axis=1)
todo

todo = todo.drop(['Customer ID'],axis=1)
todo

todo['Company'] = todo['Company'].map({'Pink Cab': 0, 'Yellow Cab': 1})

todo

todo['City']= todo['City'].map( {'NEW YORK NY': 0, 'CHICAGO IL': 1, 'LOS ANGELES CA': 2, 'WASHINGTON DC': 3,'BOSTON MA': 4, 'SAN DIEGO CA': 5, 'SILICON VALLEY': 6, 'SEATTLE WA': 7, 'ATLANTA GA': 8, 'DALLAS TX': 9, 'MIAMI FL': 10, 'AUSTIN TX': 11, 'ORANGE COUNTY': 12, 'DENVER CO': 13, 'NASHVILLE TN': 14, 'SACRAMENTO CA': 15, 'PHOENIX AZ': 16, 'TUCSON AZ': 17, 'PITTSBURGH PA': 18})#.astype(int)

todo['Gender'] = todo['Gender'].map({'Female': 0, 'Male': 1})

todo

todo = todo.drop(['Users'],axis=1)
todo

todo = todo.drop(['Population'],axis=1)
todo

X = todo.drop('Company', axis=1)
Y = todo.Company

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.20)
X_val, X_test_test, Y_val, Y_test = train_test_split(X, Y, test_size=.20)
print("Datos iniciales\t\t:\t{}\n\t- X_train\t:\t{}\n\t- X_test\t:\t{}".format(len(X),len(X_train),len(X_test)))

X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=len(X_test))

print("Datos iniciales\t\t:\t{}\n\t- X_train\t:\t{}\n\t- X_val\t\t:\t{}\n\t- X_test\t:\t{}".format(len(X),len(X_train),len(X_val),len(X_test)))

from sklearn.linear_model import LinearRegression

model = LinearRegression().fit(X_train,Y_train)

Y_predict = model.predict(X_test)
plt.plot(np.array(Y_test)[:200],'b')
plt.plot(Y_predict[:200],'r')
plt.legend(['Original', 'Prediction'])
plt.show()

print("Bondad del ajuste en Train:\t{}".format(model.score(X_train,Y_train)))
print("Bondad del ajuste en Test:\t{}".format(model.score(X_test,Y_test)))

from sklearn.linear_model import Lasso
model = Lasso(alpha=1.0).fit(X_train,Y_train)

print("Bondad del ajuste en Train:\t{}".format(model.score(X_train,Y_train)))
print("Bondad del ajuste en Test:\t{}".format(model.score(X_test,Y_test)))

from sklearn import neighbors
###se utiliza los parámetros default: distancia euclídea y k=5
model = neighbors.KNeighborsClassifier().fit(X_train, Y_train)

#predicción sobre la partición de test
Y_predict = model.predict(X_test)

from sklearn.metrics import accuracy_score
acc_sklearn = accuracy_score(Y_test, Y_predict)

print("Acc (sklearn): {}".format(acc_sklearn))

from sklearn import tree

#árbol de decisión importado con parámetros default
model = tree.DecisionTreeClassifier().fit(X_train,Y_train)

Y_predict = model.predict(X_test)

print("Acc: {}".format(accuracy_score(Y_test, Y_predict)))

from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn import svm

model = svm.SVC().fit(X_train,Y_train)

Y_predict = model.predict(X_test)
print("Acc: {}".format(accuracy_score(Y_test, Y_predict)))

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier().fit(X_train,Y_train)
Y_predict = model.predict(X_test)
print("Acc: {}".format(accuracy_score(Y_test, Y_predict)))

import pandas as pd
from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25)

histogr = {i:list(Y).count(i) for i in set(Y)}
print("La distribución inicial de clases es: {}".format(histogr))

todo.hist()

from sklearn.metrics import precision_score,recall_score,f1_score

#Importamos el método para la validación cruzada:
from sklearn.model_selection import cross_val_score

#Creamos un clasificador (árbol de decisión con parámetros por defecto):
todo_CV = tree.DecisionTreeClassifier()

#Realizamos la validación cruzada con k=10 particiones:
scores = cross_val_score(todo_CV, X, Y, cv=10)

#Mostramos los resultados por cada partición:
print(scores)

#Incluimos el método make_scorer, cambias la métrica porque se suele usar sklearn accuracy pero maker score balancea las clases:
from sklearn.metrics import make_scorer

#Realizamos la misma validación cruzada con k=10 particiones pero usando la métrica F1 sobre la clase 0:
scores = cross_val_score(todo_CV, X, Y, cv=10, scoring=make_scorer(f1_score, average='weighted'))

#Mostramos los resultados por cada partición:
print(scores)

from sklearn.model_selection import KFold
from sklearn.metrics import f1_score
from sklearn.metrics import multilabel_confusion_matrix
import matplotlib.pyplot as plt

kf = KFold(n_splits=10)
kf.get_n_splits(X)

print(kf)
k=0
for train_index, test_index in kf.split(X):
    #print("TRAIN:", train_index, "TEST:", test_index)
    X_train, X_test = X.loc[train_index], X.loc[test_index]
    y_train, y_test = Y.loc[train_index], Y.loc[test_index]
    model = RandomForestClassifier().fit(X_train, y_train)
    Y_predict = model.predict(X_test)
    print('fold: ',k)
    print("F1 score: {}".format(f1_score(y_test, Y_predict,pos_label=1, average='binary')))
    print("Acc: {}".format(accuracy_score(y_test, Y_predict)))
    fig, (ax1, ax2) = plt.subplots(1, 2)
    
    ax1.imshow(multilabel_confusion_matrix(y_test, Y_predict)[0], cmap='binary')
    ax1.set_title('label: 0')
    for i in range(2):
        for j in range(2):
            text = ax1.text(j, i, multilabel_confusion_matrix(y_test, Y_predict)[0][i, j],
                       ha="center", va="center", color="g")
    ax2.imshow(multilabel_confusion_matrix(y_test, Y_predict)[1], cmap='binary')
    ax2.set_title('label: 1')
    for i in range(2):
        for j in range(2):
            text = ax2.text(j, i, multilabel_confusion_matrix(y_test, Y_predict)[1][i, j],
                       ha="center", va="center", color="g")
    plt.show()
    
    TN = multilabel_confusion_matrix(y_test, Y_predict)[1][0,0]
    FP = multilabel_confusion_matrix(y_test, Y_predict)[1][0,1]
    FN = multilabel_confusion_matrix(y_test, Y_predict)[1][1,0]
    TP = multilabel_confusion_matrix(y_test, Y_predict)[1][1,1]
    
    recall = TP/(TP+FN)
    precision = TP/(TP+FP)
    
    f1_calculado = 2 * (precision * recall) / (precision + recall)
    print('F1 calculado', f1_calculado)
    
    k += 1

kf = KFold(n_splits=10, shuffle = True)
kf.get_n_splits(X)

print(kf)
k= 1
F1 = 0
acc = 0
for train_index, test_index in kf.split(X):
    #print("TRAIN:", train_index, "TEST:", test_index)
    X_train, X_test = X.loc[train_index], X.loc[test_index]
    y_train, y_test = Y.loc[train_index], Y.loc[test_index]
    model = neighbors.KNeighborsClassifier().fit(X_train, y_train)
    Y_predict = model.predict(X_test)
    print('fold: ',k)
    print("F1 score: {}".format(f1_score(y_test, Y_predict,pos_label=1, average='binary')))
    print("Acc: {}".format(accuracy_score(y_test, Y_predict)))
    F1 += f1_score(y_test, Y_predict,pos_label=1, average='binary')
    acc += accuracy_score(y_test, Y_predict)

    k += 1

print('mean F1', F1/k)
print('mean Acc', acc/k)

from sklearn.decomposition import PCA

from sklearn.cluster import KMeans

cluster_range = range( 1, 10 )
cluster_errors = []

for num_clusters in cluster_range:
  clusters = KMeans( num_clusters,n_init = 10, random_state=2)
  clusters.fit(todo)
  labels = clusters.labels_
  centroids = clusters.cluster_centers_
  cluster_errors.append( clusters.inertia_ )
clusters_df = pd.DataFrame( { "num_clusters":cluster_range, "cluster_errors": cluster_errors } )
clusters_df[0:10]

plt.figure(figsize=(12,6))
plt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = "o" )

kmeans = KMeans(n_clusters=4, n_init = 20, random_state=4)
kmeans.fit(todo)

centroids=kmeans.cluster_centers_


# creating a new dataframe only for labels and converting it into categorical variable
df_labels = pd.DataFrame(kmeans.labels_ , columns = list(['labels']))

df_labels['labels'] = df_labels['labels'].astype('category')

model_clus5 = KMeans(n_clusters = 2, max_iter=50)
model_clus5.fit(todo)

kmeans = KMeans(n_clusters=2,random_state=42)
clusters = kmeans.fit_predict(todo)
df_k = todo.copy(deep=True)
df_k['label'] = clusters

df_k

df_k[df_k.loc[:,'label']==0].mean()

df_k[df_k.loc[:,'label']==1].mean()

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import numpy as np

def reduce_data(data):
    scaler = StandardScaler()
    scaler.fit(data)
    X = scaler.transform(data)
    pca = PCA(n_components=2)
    X = pca.fit_transform(X)
    return X, pca

def get_kmeans(data):
    kmeans = KMeans(n_clusters=2)
    kmeans.fit(data)
    centroids = kmeans.cluster_centers_
    labels = kmeans.labels_
    
    return centroids, labels

def biplot(data, directions, labels, country_names, centroids, var_names):
    xs = data[:,0]
    ys = data[:,1]
    
    scalex = 1.0/(xs.max() - xs.min())
    scaley = 1.0/(ys.max() - ys.min())
    
    plt.tight_layout()

def biplot(data, directions, labels, country_names, centroids, var_names):
    xs = data[:,0]
    ys = data[:,1]
    
    scalex = 1.0/(xs.max() - xs.min())
    scaley = 1.0/(ys.max() - ys.min())
    
    plt.tight_layout()  
    
    # plot country points
    plt.scatter(xs * scalex,ys * scaley, c = labels, s=200)
    # plot centroids 
    plt.scatter(centroids[:, 0] * scalex, centroids[:, 1] * scaley, c='r', marker='x', s=500)
    
    # print country names 
    for i in range(len(Company_names)):
        plt.text(xs[i] * scalex, ys[i] * scaley + 0.01, Company_names[i], color = 'black', fontsize=12)
    
    # draw variable directions
    for i in range(directions.shape[0]):
        if i != 1:
            plt.arrow(0, 0, directions[i,0] * 0.3, directions[i,1] * 0.3, color = 'r',alpha = 0.5)
            plt.text(directions[i,0]* 0.35, directions[i,1] * 0.35, var_names[i], color = 'g', fontsize=10, ha = 'center', va = 'center')

def biplot(data, directions, labels, country_names, centroids, var_names):
 xs = data[:,0]
 ys = data[:,1]
    
 scalex = 1.0/(xs.max() - xs.min())
 scaley = 1.0/(ys.max() - ys.min())

 plt.tight_layout()  
 plt.scatter(xs * scalex, ys * scaley, c = labels, s=200)
 plt.scatter(centroids[:, 0] * scalex, centroids[:, 1] * scaley, c='r', marker='x', s=500)
 for i in range(len(todo_names)):
  plt.text(xs[i] * scalex, ys[i] * scaley + 0.01, todo_names[i], 
          color = 'black', fontsize=12)
 for i in range(directions.shape[0]):
  if i != 1:
   plt.arrow(0, 0, directions[i,0] * 0.3, directions[i,1] * 0.3, 
            color = 'r', alpha = 0.5)
   plt.text(directions[i,0]* 0.35, directions[i,1] * 0.35, var_names[i], color = 'g', fontsize=10, ha = 'center', va = 'center')

import tensorflow as tf

#Importamos el método:
from sklearn.cluster import KMeans

#Inicializamos el algoritmo pasando los parámetros especificados:
kmeans = KMeans(n_clusters=2, init='random')

#Realizamos el proceso de clustering sobre los datos X_clust:
clustering = kmeans.fit(X)

#Comprobamos que únicamente hay tantos clusters como se ha especificado:
print("El número de clusters diferentes es: {}\n".format(clustering.n_clusters))

#Por otro lado vamos a comprobar las asignaciones de los 10 primeros elementos:
print("Assignments of the first 10 elements:")
for it_sample in range(10):
  print("\t- Element {} from X_test is assigned to cluster {}".format(it_sample,clustering.labels_[it_sample]))

#Inicializamos PCA a una salida de 2 dimensiones:
PCA_Mapping = PCA(n_components=2).fit(X)

#Mapeamos los datos:
X_clust_PCA = PCA_Mapping.transform(X)

#Inicializamos el algoritmo (5 clusters) y realizamos el clustering:
clustering_PCA = KMeans(n_clusters = 2, init='k-means++').fit(X_clust_PCA)

#Importamos la librería gráfica:
import matplotlib.pyplot as plt

#Vector de colores:
colors = ['blue','red']

# #Mostramos los datos en crudo:
# plt.subplot(2,1,1)
# plt.scatter(X_clust_PCA[:100,0], X_clust_PCA[:100,1])
# plt.subplot(2,1,2)

#Mostramos los datos, siendo el color lo que representa su pertenencia a cada cluster:
plt.scatter(X_clust_PCA[:100,0], X_clust_PCA[:100,1], c=[colors[u] for u in clustering_PCA.labels_[:100]])

#Mostramos los centroides de cada uno de los clusters:
plt.scatter(clustering_PCA.cluster_centers_[:,0], clustering_PCA.cluster_centers_[:,1], c=colors, marker='x', s=1000)


plt.show()