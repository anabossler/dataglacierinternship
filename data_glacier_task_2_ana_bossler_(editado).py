# -*- coding: utf-8 -*-
"""Data Glacier Task 2 Ana Bossler (editado).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H5JsXlW-Rhyu-rsUKclEkc93IfLFpVtl
"""

import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np
import sklearn
# Commented out IPython magic to ensure Python compatibility.
import seaborn as sns
# %matplotlib inline

print("Versión de librería sklearn: {}".format(sklearn.__version__))

datacab = pd.read_csv('Cab_Data.csv', header=0)
datacity = pd.read_csv('City.csv', header=0)
datacustomer = pd.read_csv('Customer_ID.csv', header=0)
datatransaction = pd.read_csv('Transaction_ID.csv', header=0)

datacab.head()

#costo medio de viaje
#costo medio de viaje por km
#costo medio de viaje en función de la ciudad
#número de viajes

datacab_pink = datacab[datacab['Company'] == 'Pink Cab']
datacab_pink.mean()

datacab_yellow = datacab[datacab['Company'] == 'Yellow Cab']
datacab_yellow.mean()

datacab_pink.hist()

datacab_yellow.hist()

print(datacab['Cost of Trip'].mean())
print(datacab['KM Travelled'].mean())

counts = datacab['Company'].value_counts()
counts

datacity.head()

datacustomer.head()

#edad media
#income media
datacustomer.hist()

datatransaction.head()

#relacionar transaction ID con customer ID y compañía, para conocer por separado los valores para las dos compañías

print(f'The number of rows are {datacab.shape[0]} \nand the number of columns are {datacab.shape[1]}')

print(f'The number of rows are {datacity.shape[0]} \nand the number of columns are {datacity.shape[1]}')

print(f'The number of rows are {datacustomer.shape[0]} \nand the number of columns are {datacustomer.shape[1]}')

print(f'The number of rows are {datatransaction.shape[0]} \nand the number of columns are {datatransaction.shape[1]}')

datacab.describe()

datacity.describe()

datacustomer.describe()

datatransaction.describe()

datacab.corr()

datacustomer.corr()

datatransaction.corr()

sns.pairplot(datacab,diag_kind='kde')

sns.pairplot(datacustomer,diag_kind='kde')

sns.pairplot(datatransaction,diag_kind='kde')

def missing(df):
    total=df.isnull().sum().sort_values(ascending=False)
    percent=(df.isnull().sum()*100/df.isnull().count()).sort_values(ascending=False)

missing(datacab)

missing(datacity)

missing(datacustomer)

missing(datatransaction)

datacab.isnull().sum()

datacity.isnull().sum()

datacustomer.isnull().sum()

datatransaction.isnull().sum()

features=list(datacab.select_dtypes(exclude=['object']))
fig=plt.subplots(figsize=(15,30))
for i, j in enumerate(features):
    plt.subplot(6, 2, i+1),
    plt.subplots_adjust(hspace = 1.0)
    sns.boxplot(datacab[j])
    plt.title(j)

features=list(datacity.select_dtypes(exclude=['object']))
fig=plt.subplots(figsize=(15,30))
for i, j in enumerate(features):
    plt.subplot(6, 2, i+1),
    plt.subplots_adjust(hspace = 1.0)
    sns.boxplot(datacity[j])
    plt.title(j)

features=list(datacustomer.select_dtypes(exclude=['object']))
fig=plt.subplots(figsize=(15,30))
for i, j in enumerate(features):
    plt.subplot(6, 2, i+1),
    plt.subplots_adjust(hspace = 1.0)
    sns.boxplot(datacustomer[j])
    plt.title(j)

features=list(datatransaction.select_dtypes(exclude=['object']))
fig=plt.subplots(figsize=(15,30))
for i, j in enumerate(features):
    plt.subplot(6, 2, i+1),
    plt.subplots_adjust(hspace = 1.0)
    sns.boxplot(datatransaction[j])
    plt.title(j)

transactions = pd.merge(datacab, datatransaction, on='Transaction ID')
transactions

todo = pd.merge(transactions, datacustomer, on='Customer ID')
todo

plt.scatter(todo['Cost of Trip'], todo['Income (USD/Month)'],alpha=0.05)

plt.scatter(todo['Date of Travel'], todo['Age'],alpha=0.002)

plt.scatter(todo['Cost of Trip'], todo['KM Travelled'],alpha=0.005)

plt.hist(todo['Cost of Trip'])

todo

todo['City'].value_counts()

todo['City']= todo['City'].map( {'NEW YORK NY': 0, 'CHICAGO IL': 1, 'LOS ANGELES CA': 2, 'WASHINGTON DC': 3,'BOSTON MA': 4, 'SAN DIEGO CA': 5, 'SILICON VALLEY': 6, 'SEATTLE WA': 7, 'ATLANTA GA': 8, 'DALLAS TX': 9, 'MIAMI FL': 10, 'AUSTIN TX': 11, 'ORANGE COUNTY': 12, 'DENVER CO': 13, 'NASHVILLE TN': 14, 'SACRAMENTO CA': 15, 'PHOENIX AZ': 16, 'TUCSON AZ': 17, 'PITTSBURGH PA': 18})#.astype(int)

todo

todo['City'].value_counts()

todo['Gender'] = todo['Gender'].map({'Female': 0, 'Male': 1})

todo

todo['Company'] = todo['Company'].map({'Pink Cab': 0, 'Yellow Cab': 1})
todo

todo = todo.drop(['Payment_Mode'],axis=1)
todo

todo = todo.drop(['Transaction ID', 'Customer ID'],axis=1)
todo

target = todo.pop('Company')

import tensorflow as tf
dataset = tf.data.Dataset.from_tensor_slices((todo.values, target.values))

for feat, targ in dataset.take(5):
  print ('Features: {}, Target: {}'.format(feat, targ))

train_dataset = dataset.shuffle(len(todo)).batch(64)

def get_compiled_model():
    model = tf.keras.Sequential([
    tf.keras.layers.Dense(8, activation='relu'),
    tf.keras.layers.Dense(10, activation='relu'),
    tf.keras.layers.Dense(1)
  ])

    model.compile(optimizer='adam',
                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                metrics=['accuracy'])
    return model

model = get_compiled_model()
model.fit(train_dataset, epochs=15)

predictions = model.predict( 'aqui datos' )
print(predictions)